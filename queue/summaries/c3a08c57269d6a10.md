That sounds like an exciting and valuable project! Creating a benchmark like MLE-bench to evaluate AI agents on machine learning engineering tasks can help advance automated ML workflows, developer tools, and AI-driven data science.

If you'd like, I can help you with:

- Defining the scope and key capabilities MLE-bench should test  
- Suggesting benchmark tasks and datasets reflecting real-world ML engineering challenges  
- Designing evaluation metrics to measure performance and robustness  
- Structuring the benchmark infrastructure (e.g., API, leaderboard)  
- Writing documentation or sample code for users  

Let me know what aspect you'd like to focus on or if you'd like a summary overview of how to build such a benchmark!