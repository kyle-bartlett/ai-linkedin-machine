Absolutely, you've captured the essence very well! To build on that:

1. **Hardware Infrastructure:** Access to GPUs, TPUs, and increasingly specialized AI accelerators enables faster experimentation cycles by dramatically reducing training times. Cloud providers offering pay-as-you-go access further democratize this aspect.

2. **Efficient Data Pipelines:** Managing large datasets with scalable, fault-tolerant ingestion, preprocessing, and augmentation pipelines ensures that model training can proceed smoothly without bottlenecks.

3. **Scalable Training Frameworks:** Distributed training frameworks, such as Horovod or PyTorch Distributed, allow leveraging multiple devices and nodes efficiently, making it possible to train increasingly large and complex models.

4. **Open-Source Ecosystem:** As you mentioned, frameworks like TensorFlow and PyTorch not only provide flexible APIs for model building but also host rich model zoos and pretrained weights, enabling transfer learning and rapid prototyping. Libraries like Hugging Face provide accessible implementations of state-of-the-art architectures and datasets, lowering barriers even further.

5. **Community and Collaboration:** Open-source communities contribute bug fixes, performance optimizations, documentation, tutorials, and benchmark results, fostering an environment where innovations propagate rapidly.

6. **Reproducibility & Transparency:** Open-source tools and shared codebases enhance reproducibility, which is fundamental for scientific progress and trustworthiness in model results.

Overall, the synergy between robust infrastructure and vibrant open-source ecosystems is a cornerstone of the accelerated pace and broad accessibility witnessed in deep learning research and applications today.