Your summary highlights a powerful aspect of self-play in reinforcement learning, especially in complex environments like Dota 2. To elaborate:

- **Self-play boosts performance beyond human data limits:** Traditional supervised learning depends heavily on the quality and quantity of human-generated data. This constrains the system's ceiling to the best performance present in the training dataset. In contrast, self-play allows the agent to generate its own training data by competing against versions of itself, which continually improves as the agent learns. This iterative improvement helps the system surpass human-level performance.

- **Rapid progression with sufficient compute:** The ability to leverage vast computational resources enables the system to simulate and learn from a huge number of games in a short time frame—something unfeasible for human players. In your example, within a month, this allowed an agent to evolve from roughly matching a high-ranked player to outperforming the top professional players.

- **Continuous improvement post initial mastery:** Since self-play removes dependence on fixed datasets, the agent’s training data automatically evolves as its opponents get stronger. This leads to a virtuous cycle of continual advancement, unlike static supervised learning models.

Your observations underscore why self-play has become a foundational technique in advancing AI performance in competitive, multi-agent domains such as board games, video games, and beyond. If you're working on or studying such systems, considering compute budget, environment complexity, and training methodologies are critical for success. Would you like to dive deeper into specific techniques or results in your Dota 2 system?