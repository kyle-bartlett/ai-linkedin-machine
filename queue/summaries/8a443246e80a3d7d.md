The paper with arXiv ID 2505.15557v3 presents a novel approach to improving Gaussian processes (GPs) for modeling data that exhibit sudden changes or "jumps." Traditional GPs assume stationarity, which limits their ability to handle such discontinuities effectively. The previously proposed "jump GP" (JGP) model tackled this issue by combining local GPs with latent "level" variables in a joint inference framework, but this joint modeling is complex and computationally challenging.

This work proposes a more modular alternative that avoids joint inference yet preserves the core beneficial ideas of the JGP:

1. Learning optimal neighborhood sizes to adapt locally around the manifold where discontinuities reside.
2. Introducing a cluster-based latent feature to represent distinct output levels on either side of the discontinuity.

They demonstrate that each of these components independently improves the modeling of jump processes significantly. When combined, these modular components yield even better performance without the difficulties associated with joint inference. The approach is validated on synthetic and real benchmark datasets, showing strong results.

In summary, this paper contributes a simpler, effective methodology for handling discontinuities in Gaussian process regression, improving flexibility and computational tractability.