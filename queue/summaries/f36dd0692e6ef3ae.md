The paper with arXiv ID 2405.05075v4 presents a study on generating and defending against sparse adversarial perturbations in machine learning models. The key contributions include:

- Introduction of **Sparse-PGD**, a PGD-like white-box attack method designed to efficiently produce both unstructured and structured sparse adversarial perturbations.
- A framework combining Sparse-PGD with a black-box attack, enabling a more comprehensive and reliable robustness evaluation against sparse perturbations.
- Leveraging the efficiency of Sparse-PGD to perform adversarial training, resulting in models with enhanced robustness to a variety of sparse attacks.
- Extensive experimental validation showing the superior performance of Sparse-PGD in producing sparse adversarial examples.
- Demonstration that models adversarially trained with this approach achieve **state-of-the-art robustness** compared to other robust models under various sparse attack scenarios.

The implementation code is publicly available at:  
https://github.com/CityU-MLO/sPGD

If you want, I can provide more details about the methodology, experimental setup, or implications of this work.