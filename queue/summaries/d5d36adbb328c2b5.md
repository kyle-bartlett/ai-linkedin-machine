Yes, evolution strategies (ES) have indeed shown promising results in reinforcement learning tasks, particularly in continuous control problems like those found in MuJoCo and even in some discrete tasks such as Atari games. Here's a bit more detail on why ES is competitive and what advantages it offers compared to traditional RL methods:

### What are Evolution Strategies (ES)?
Evolution Strategies are a type of black-box optimization inspired by natural evolution. Instead of relying on gradients and backpropagation, ES optimizes policy parameters by:

1. **Sampling**: Perturbing the current policy parameters with noise to generate a population of candidate solutions.
2. **Evaluation**: Assessing the performance (fitness) of each candidate on the task.
3. **Update**: Adjusting parameters in the direction that leads to higher fitness using the weighted sum of perturbations.

### Why ES is Competitive on RL Benchmarks
- **Gradient-free optimization**: ES doesn't require differentiability or gradients of the environment or policy. This is beneficial when gradients are noisy, sparse, or unavailable.
- **Parallelization**: ES can easily scale by running many evaluations of candidate policies concurrently, making it suitable for distributed computing.
- **Robustness to reward shaping and exploration**: ES often explores the parameter space more broadly rather than relying on noisy reward signals to propagate gradients.
- **Simpler implementation**: ES does not require complex algorithms like value function approximation, temporal difference learning, or policy gradients, making it easier to implement and debug.

### Limitations and Challenges
- **Sample inefficiency**: ES typically requires more environment interactions compared to some model-free RL methods.
- **Scaling to high dimensions**: While ES scales well to some extent, very high-dimensional parameters might slow down or degrade performance without careful tuning.
- **Delayed reward handling**: RL algorithms with temporal credit assignment (like PPO or DDPG) can exploit the structure of sequential decision making more directly than ES.

### References and Notable Work
- The paper "**Evolution Strategies as a Scalable Alternative to Reinforcement Learning**" by Salimans et al. (2017) demonstrated that ES could solve MuJoCo and Atari environments competitively.
- Recent hybrid approaches combine ES with gradient-based RL to leverage the strengths of both.

### Summary
While reinforcement learning remains dominant in many applications, evolution strategies provide a powerful and often overlooked alternative, especially when simplicity, parallelism, and gradient-free optimization are advantageous. This has renewed interest in ES for modern RL challenges.

If youâ€™d like, I can also help you with code examples, comparisons, or deeper dives into specific ES methods!