That's a great summary! Double descent is indeed reshaping how we think about generalization in deep learning beyond classical statistics.

If you’d like, here are a few pointers to seminal and recent papers that explore double descent both theoretically and empirically:

### Key Papers on Double Descent
- **Belkin et al., 2019 — "Reconciling modern machine-learning practice and the classical bias–variance trade-off"**  
  Introduced the double descent phenomenon in a clear, accessible manner, demonstrating it on random feature models and decision trees.  
  [Link (PNAS)](https://www.pnas.org/doi/10.1073/pnas.1903070116)

- **Nakkiran et al., 2020 — "Deep Double Descent: Where Bigger Models and More Data Hurt"**  
  This paper conducted extensive empirical studies on deep neural networks (including ResNets, CNNs) showing double descent behavior with respect to model size, training epochs, and data size.  
  [Link (ICLR 2020)](https://arxiv.org/abs/1912.02292)

- **Advani et al., 2020 — "A high-dimensional theory of generalization dynamics under gradient descent for overparameterized neural networks"**  
  Offers theoretical analysis for generalization dynamics in high dimensions connecting with double descent.  
  [Link (NeurIPS 2020)](https://arxiv.org/abs/1908.08974)

- **Geiger et al., 2020 — "Scaling description of generalization with number of parameters in deep learning"**  
  Investigates transitions in double descent related to scaling laws and implicit regularization.  
  [Link](https://arxiv.org/abs/2102.11418)

### Other Helpful References
- **Belkin, 2021 — "The Role of Interpolation in Modern Machine Learning" (JMLR Special Issue)**  
  A broader review covering interpolation, overparameterization, and generalization.  
  [Link (JMLR)](http://jmlr.org/papers/v23/21-0064.html)

- **Nakkiran's Blog & Talks**  
  Practical explanations and intuition, often excellent for grasping core ideas.

---

If you're interested, I can also suggest:  

- How to experimentally observe double descent with your own models.  
- Tips on training regimes or regularization schemes to highlight or avoid the interpolation peak.  
- Detailed explanations of theoretical frameworks like random matrix theory or neural tangent kernels as they relate to double descent.

Would you like me to provide code examples or detailed walkthroughs of any aspect?