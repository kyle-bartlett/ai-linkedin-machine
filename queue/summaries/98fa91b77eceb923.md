It sounds like you're describing an approach based on **preference learning** or **inverse reinforcement learning (IRL)**, where instead of specifying a fixed reward function or goal manually, the AI learns what humans value by being given comparisons between different behaviors or outcomes. By iteratively choosing the preferred option according to human input, the system can infer an implicit reward model that better represents human intentions, reducing the risk of misalignment or unintended consequences.

DeepMind and other research groups have explored related ideas like **Cooperative Inverse Reinforcement Learning (CIRL)** and **preference-based reinforcement learning**. These methods aim to align AI systems more closely with what humans actually want, especially when directly specifying the goal is hard or risky.

If you want, I can provide more details on how such algorithms work, or examples of recent research in this area. Would that be helpful?