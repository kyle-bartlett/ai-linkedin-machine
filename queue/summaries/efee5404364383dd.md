The paper with arXiv ID **2511.16655v1** performs a critical evaluation of **Cambrian-S**, a system proposed to improve video world models through **spatial supersensing**. Here are the key points and findings from the abstract:

1. **Cambrian-S Contributions:**
   - Introduced two benchmarks:  
     - **VSI-Super-Recall (VSR)**  
     - **VSI-Super-Counting (VSC)**
   - Developed predictive sensing inference strategies tailored to those benchmarks.

2. **Critical Analysis Provided:**
   - **Simple Baseline (NoSense):**  
     - Uses almost no temporal structure, relying only on a "bag-of-words" SigLIP model.  
     - Despite the simplicity, achieves 95% accuracy on VSR (even with 4-hour-long videos).  
     - This suggests that VSR can be solved without actual spatial cognition or supersensing, questioning its effectiveness.

   - **Shortcut Heuristics in Inference:**  
     - Cambrian-S's inference method for VSC appears to exploit shortcuts instead of robust spatial supersensing.  
     - The authors propose a **sanity check (VSC-Repeat):** concatenating videos with themselves multiple times, which doesn't change the unique object count.  
     - Cambrian-S accuracy collapses from 42% to 0% under this test, showing failure to recognize repeated scenes and maintain consistent object counts.  
     - Indicates reliance on the implicit benchmark assumption that rooms are never revisited.

3. **Conclusions:**
   - Current VSI-Super benchmarks (VSR and VSC) don't reliably measure spatial supersensing capabilities.
   - Cambrian-S's improved performance likely comes from exploiting benchmark shortcuts rather than genuine spatial understanding or predictive sensing.

4. **Additional Information:**
   - The paper includes a response from the original Cambrian-S authors in an appendix to provide a balanced view.
   - The authors provide open-source code for reproducing and extending their experiments:  
     https://github.com/bethgelab/supersanity

---

If you want, I can help summarize more details, discuss implications, or find related work on spatial supersensing or video world models.