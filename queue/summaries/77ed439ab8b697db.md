The paper described in arXiv:2511.13465v3 proposes **AdamNX**, a novel optimization algorithm designed to improve upon Adam by addressing its tendency to converge to sharp (non-flat) minima, which can negatively impact generalization.

### Key points from the abstract:
- **Context**: 
  - Adam is widely used for training large models, including large language models.
  - However, compared to SGD-based methods, Adam tends to settle in sharp minima, which can cause worse generalization.

- **Innovation**:
  - AdamNX introduces a novel **second-order moment estimation exponential decay rate**.
  - This decay rate gradually reduces the learning step correction strength over training.
  - In the stable training phase, AdamNX effectively behaves like momentum SGD.
  - This change aims to enhance training stability in later phases and improve generalization performance.

- **Results**:
  - Experimental results show AdamNX outperforms Adam and its variants.
  - The new exponential decay formulation for the second-order moment estimation is more effective than currently used ones.

- **Code release**:
  - The authors provide open-source code: https://github.com/mengzhu0308/AdamNX

---

### Summary:

**AdamNX** is an improved optimizer based on Adam that transitions toward momentum SGD behavior during stable training phases through a novel adaptive decay rate for second-moment estimation. This helps avoid sharp minima and potentially yields better generalization. It is validated experimentally and available as open-source.

If you'd like, I can help summarize the paper further, explain how AdamNX works in detail, or discuss how it compares to other optimizers!