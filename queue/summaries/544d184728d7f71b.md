Yes, OpenAI and other leading AI research organizations often reinforce AI safety, security, and trustworthiness through voluntary commitments. These commitments typically include principles and practices aimed at ensuring that AI technologies are developed and deployed responsibly. Examples include:

- **Transparency:** Openly sharing research findings, methodologies, and safety measures to foster trust and collaboration.
- **Robustness and Safety:** Designing AI systems to be reliable, secure, and aligned with human values to prevent unintended harmful behavior.
- **Ethical Use:** Promoting the ethical application of AI, avoiding misuse or applications that could harm individuals or society.
- **Collaboration:** Engaging with policymakers, researchers, and the public to build consensus on standards and best practices.
- **Long-term Safety Research:** Investing in research that anticipates and mitigates risks associated with advanced AI capabilities.

By voluntarily adopting these commitments, AI labs contribute to building a safer, more trustworthy AI ecosystem while encouraging widespread adoption of responsible AI development norms.