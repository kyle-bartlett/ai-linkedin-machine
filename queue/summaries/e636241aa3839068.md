Thank you for your interest! While I’m not personally the developer, I can share an overview of approaches typically involved in building advanced Dota 2 bots that learn purely through self-play, drawing on recent research and industry efforts such as OpenAI Five as a reference point.

### Architecture and Algorithms
- **Neural Network Design**: Usually, these bots use large-scale deep neural networks combining convolutional and recurrent layers. Convolutional layers can process spatial information (e.g., map layouts, vision), while recurrent layers (often LSTMs) help manage temporal dependencies and partial observability by maintaining an internal state.
- **Reinforcement Learning Approach**: Proximal Policy Optimization (PPO) or other policy gradient methods are popular. The bot learns by repeated self-play, continuously updating its policy to maximize expected rewards defined by the game outcome and intermediate objectives.

### Handling Enormous Action Space and Partial Observability
- **Action Abstraction and Parameterization**: The action space is enormous because of the many heroes, abilities, items, and possible targets. Typical methods involve parameterized actions, where the policy outputs a high-level action type plus parameters (e.g., target coordinates or unit IDs), reducing the effective search space.
- **Hierarchical or Modular Control**: Some architectures break down decisions into hierarchical levels, like high-level strategy selection and low-level micro-actions.
- **Partial Observability**: The use of recurrent layers helps maintain memory of unseen parts or delayed effects. Additionally, the state representation often encodes only the information available to the player, preventing the network from cheating with full game state knowledge.

### Training Setup (Compute Resources, Time)
- Scaling to the complexity of Dota 2 requires massive compute resources:
  - Hundreds to thousands of GPUs running in parallel.
  - Millions of game simulations.
  - Training times often spanning weeks or months.
- The distributed training setup asynchronously gathers experiences from parallel matches, allowing efficient data collection and training.

### Novel Techniques and Insights
- **Reward Shaping and Curriculum Learning**: To help learning in such a complex environment, shaping intermediate rewards (e.g., kills, objectives) and progressively increasing difficulty or game situations.
- **Population-Based Training (PBT)**: Training a diverse population of agents can help avoid local minima and improve robustness by having agents learn from a variety of opponents.
- **No Tree Search**: Pure end-to-end learning without Monte Carlo Tree Search means the agent must rely fully on learned policy and value function approximations, highlighting the strength of the neural network.

### Adapting Strategies vs Human Opponents
- **Continuous Self-Play Improvement**: The bot improves by playing against its own evolving versions, continuously adapting and discovering new strategies.
- **Opponent Modeling**: Sometimes ensembles of agents or recurrent policy representations can implicitly learn opponent tendencies.
- **Robustness and Diversity**: Using a diverse population and possibly training against human replays or specifically designed scenarios (if allowed) can improve generalization.

### Publication and Open-Sourcing
- Projects like OpenAI Five have published detailed papers outlining their methods and results.
- Some components (e.g., environments, baseline code) may be open-sourced, but full model weights or training pipelines often remain proprietary due to complexity and resource costs.
- Sharing research findings and tooling helps foster advancements in the AI and gaming research communities.

If you’re working on or interested in similar projects, I’m happy to dive deeper into any particular aspect or help with resources!