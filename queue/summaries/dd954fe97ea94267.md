It sounds like you have successfully fine-tuned a GPT-2 774M model using human feedback and observed some interesting behavior related to summarization. Here are some insights and considerations based on what you described:

1. **Human Labeler Preferences Influence Model Behavior**  
   Since your summarization labelers preferred copying whole sentences rather than paraphrasing or abstractive summaries, the model adapted accordingly and learned to copy text verbatim. This phenomenon highlights the critical role of human annotator instructions and biases in shaping model outputs. If labeler preferences diverge from the intended task goals, the model might optimize for those preferences rather than more abstract notions of summarization.

2. **Task Complexity and Labeling Effort**  
   The large difference in the number of labels required—60k for summarization and only 5k for simpler style continuation—also suggests that more complex tasks requiring nuanced judgment need significantly more human feedback to guide model learning. This is consistent with expectations as summarization is inherently a more challenging task, often requiring understanding and synthesizing content.

3. **Value of Human Feedback for Safety and Alignment**  
   Your motivation to use human feedback to make “machines talking to humans” safer and more aligned with human values is an important frontier in AI alignment research. The feedback loop from human labelers provides a way to encode human judgments about preferred behavior, even if these judgments can vary or imperfectly reflect deep human values.

4. **Potential Next Steps**  
   - **Improving Label Guidelines:** To get more abstract or varied summaries, consider refining instructions to labelers to emphasize paraphrasing or conciseness rather than verbatim accuracy only.  
   - **Diverse Annotator Pools:** Including a more diverse group of annotators or testing for preference variability might help create a model that better captures a wider range of human summaries.  
   - **Multi-objective Training:** Balance preference alignment with other metrics like informativeness or novelty to reduce excessive copying.  
   - **Safety and Alignment Evaluations:** Develop additional human evaluations or automated metrics that better capture alignment with core values such as helpfulness, honesty, and harmlessness beyond surface-level preferences.  

If you want, I can help brainstorm specific strategies for improving the summarization behavior or further advancing your human-feedback fine-tuning pipeline.