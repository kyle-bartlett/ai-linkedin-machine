It sounds like you're describing CLIP (Contrastive Language–Image Pre-training), a neural network developed by OpenAI that learns visual concepts directly from natural language descriptions. By training on image and text pairs, CLIP can perform zero-shot image classification—meaning it can recognize and categorize images without task-specific training—by simply being given the category names in natural language.

If you'd like, I can provide more details on how CLIP works, its architecture, training process, or examples of its applications. Just let me know!