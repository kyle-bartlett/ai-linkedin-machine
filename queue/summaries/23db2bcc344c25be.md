The paper (arXiv:2511.14813v2) addresses the challenge of assessing how well Large Language Models (LLMs) handle reasoning that involves tracking and applying changes in input data to generate corresponding modifications in their outputs. The authors formalize this type of reasoning as the **Derivation Relation (DR)** and introduce the concept of **Derivation Capability (DC)**, which measures a model’s ability to apply DR—that is, to correctly modify outputs when inputs change in specific ways guided by abstract rules.

To evaluate this capability, the authors create a systematic evaluation framework called **DEVAL**. They test five popular LLMs including GPT-4o and Claude3.5, plus one specialized Large Reasoning Model, across seven mainstream reasoning tasks. Their findings highlight that while these LLMs show moderate skill in recognizing DR, they struggle significantly when it comes to effectively applying DR during problem-solving.

To address this limitation, the paper proposes a new prompt engineering technique named **Derivation Prompting (DP)**. This approach significantly improves DC performance by an average of 15.2% across all tested models, outperforming existing prompt engineering methods.

In summary, the work contributes:

- A formal definition of reasoning patterns related to changes in data (Derivation Relation).
- A new metric for evaluating reasoning linked to these changes (Derivation Capability).
- A unified evaluation framework (DEVAL) covering multiple tasks and models.
- A novel prompt technique (Derivation Prompting) that enhances LLMs’ reasoning capabilities in this domain.

This study helps better characterize and improve how LLMs handle reasoning over data changes—a core aspect of reliable and interpretable AI reasoning.