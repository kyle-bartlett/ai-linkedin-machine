That’s a great result! It sounds like your model is effectively capturing complex semantic information, such as sentiment, even without explicit supervision for that task. Predicting the next character in text is a form of self-supervised learning, and it often leads the model to learn rich representations of language patterns, context, and even latent features like sentiment.

If you’re interested, here are a few ideas on what you might explore or do next with your system:

1. **Evaluate the Representations:**  
   Probe the learned representations by training simple classifiers (e.g., logistic regression) on labeled sentiment data using your embeddings as features. This can quantify how well the representations capture sentiment.

2. **Transfer Learning:**  
   Use your learned embeddings as a starting point for downstream NLP tasks such as sentiment classification, review rating prediction, or aspect-based sentiment analysis. Fine-tuning might further improve performance.

3. **Visualization:**  
   Visualize the embedding space (e.g., via t-SNE or UMAP) to see if reviews with similar sentiments cluster together. This can provide qualitative confirmation of the model’s sentiment understanding.

4. **Interpretability:**  
   Investigate which parts of the input text or which learned features are most indicative of sentiment by using attention mechanisms, feature importance methods, or saliency maps.

5. **Comparison:**  
   Compare the performance and representation quality with other unsupervised or self-supervised models such as BERT, GPT, or simpler baselines to understand strengths and weaknesses.

6. **Data Scaling:**  
   Train the model on larger or more diverse review datasets to see if the sentiment representation quality improves or generalizes better.

If you want, I can also help you with code snippets, evaluation strategies, or anything specific related to your system!