The paper titled **"Weighted Integrated Gradients" (WG)** addresses a fundamental limitation of the popular explainability method **Integrated Gradients (IG)** in computer vision: the choice of baseline images highly influences attribution outcomes. Traditional multi-baseline methods like **Expected Gradients (EG)** use uniform weighting over baselines, assuming all baselines contribute equally, which can introduce noise and instability in explanations, especially in high-dimensional models.

### Key Contributions:
- **Weighted Integrated Gradients (WG):** Introduces a principled, unsupervised method to evaluate and assign weights to baseline images, adapting the baseline selection on a per-input basis.
- **Preserves IG Properties:** WG maintains the original axiomatic guarantees of IG while enhancing theoretical robustness compared to EG.
- **Improved Attribution Fidelity:** Empirical results on standard image datasets show 10-35% improvement in attribution quality over EG.
- **Baseline Subset Selection:** WG can identify and emphasize the most informative baselines, reducing variability and improving explanation clarity.

### Impact:
WG challenges the conventional approach of treating all baselines equally in multi-baseline attribution methods, offering a more reliable, interpretable, and theoretically sound strategy for explaining computer vision models. This advances practical usability in explainable AI by enhancing both the stability and accuracy of attributions provided to end-users and researchers.

If you want, I can help summarize the methodology, experimental setup, or discuss implications/applications related to this work.