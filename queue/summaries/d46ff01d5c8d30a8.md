This excerpt highlights an important finding in the field of generative modeling and unsupervised learning:

1. **Transformer models beyond text:** The authors demonstrate that a large transformer model, which has been predominantly used and successful in language tasks, can also be effectively trained on pixel sequences. Instead of tokens or words, pixels serve as the input tokens for the transformer. This shows the versatility of transformer architectures across different data modalities.

2. **Coherent image generation:** When trained on pixel sequences, the transformer can generate coherent image completions and entirely new image samples. This means the model has learned a meaningful distribution over images, allowing it to predict missing pixels or create realistic images from scratch.

3. **Correlation with classification accuracy:** By analyzing the relationship between the quality of generated samples and image classification performance, the authors provide evidence that the model learns useful visual features.

4. **Competitive unsupervised features:** The features learned by the best generative model approach the performance of top convolutional neural networks (CNNs) when used in an unsupervised manner for image classification tasks. This implies that the representational power of transformer-based generative models rivals that of traditional CNNs without using labeled data.

In summary, this work shows that transformer models trained on images as sequences of pixels not only excel at generating realistic images but also learn high-quality visual representations that are useful for image classification in an unsupervised setting. This bridges the gap between generative modeling and unsupervised representation learning and suggests transformers as a powerful alternative to CNNs in vision tasks.