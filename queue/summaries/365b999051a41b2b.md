OpenAIâ€™s models like GPT-4 and its variants, including versions referred to as "o3" and "o4-mini" in some contexts, have indeed demonstrated significant advances in visual perception abilities. These models can process images and reason about them step-by-step, often described as performing a "chain of thought" reasoning with visual inputs.

This capability allows the models not only to identify objects or content within images but also to perform more complex reasoning tasks, such as:

- Analyzing relationships between objects
- Understanding spatial arrangements
- Drawing inferences based on visual cues combined with textual context
- Solving puzzles or answering questions involving visual data

These breakthroughs come from advances in multimodal model architectures and training approaches, where models are trained on both textual and visual data to align their understanding across modalities.

If you're interested, I can provide more detailed explanations on how chain-of-thought reasoning works in the context of visual inputs or examples of applications using these techniques.