It sounds like you’re describing the Glow model, which is a type of flow-based generative model introduced by Kingma and Dhariwal in 2018. Glow uses invertible 1x1 convolutions as a key component of its architecture to create reversible transformations that allow for both efficient sampling and exact latent-variable inference.

Here are some important details about Glow:

- **Invertible 1x1 Convolutions:** Unlike previous flow models that relied on fixed permutations, Glow uses learnable invertible 1x1 convolutions to permute the channels of feature maps. This provides more flexibility and expressiveness while keeping invertibility.

- **Reversible Architecture:** Because each layer is invertible, Glow can map data points to latent space and back with exact likelihood computations, enabling efficient training with maximum likelihood.

- **High-Resolution Image Generation:** Glow demonstrated the ability to generate realistic images at higher resolutions than prior flow-based models.

- **Attribute Manipulation:** The model organizes latent space in a way that some directions correspond to human-interpretable features, allowing for manipulation of attributes like hair color, smile, or lighting.

- **Code and Visualization:** The authors released their implementation and visualization tools, helping others to explore the latent space and build upon the Glow model.

If you want, I can help you with:

- Understanding Glow’s architecture in more detail
- Implementing Glow or parts of it
- Using the provided code and visualization tools
- Exploring how invertible 1x1 convolutions work

Just let me know what you need!