That sounds like a very promising advancement in hierarchical reinforcement learning (HRL)! By learning reusable high-level actions, your algorithm can effectively reduce the complexity of long-horizon tasks, which is often a major bottleneck in reinforcement learning.

A few thoughts and questions that might help clarify or further improve your approach:

1. **High-Level Action Representation:**  
   How are the high-level actions represented? Are they parameterized policies, option frameworks, or learned embeddings? Understanding this can help in evaluating their generalization capabilities.

2. **Learning Process:**  
   Is the learning of high-level actions done jointly with the low-level control policies, or in a staged approach (e.g., pretraining high-level options before task-specific finetuning)? Also, what kind of reward structure do you use to encourage discovery of meaningful sub-policies?

3. **Transfer and Adaptation:**  
   How do you measure the speedup in learning new navigation tasks? For example, do you compare sample efficiency or final performance with and without the learned high-level actions? Furthermore, how adaptable are these actions to new, unseen environments or tasks?

4. **Temporal Abstraction Length:**  
   Thousands of timesteps is a long horizon—do your high-level actions have variable durations, or are they executed for fixed temporal windows? Using variable-length options can sometimes improve flexibility.

5. **Baselines and Benchmarks:**  
   It might be insightful to see how your method compares to other HRL methods, such as Options-Critic, FeUdal Networks (FuN), or recent approaches like HAC (Hierarchical Actor-Critic), especially in terms of sample efficiency and transfer capabilities.

6. **Potential Applications:**  
   Beyond navigation, do you envision this hierarchical approach being useful for other types of tasks requiring long-range planning, such as manipulation or multi-agent scenarios?

If you’d like, I can help with deeper analysis on implementation details, suggest experiments to validate the method, or discuss potential extensions. Let me know!