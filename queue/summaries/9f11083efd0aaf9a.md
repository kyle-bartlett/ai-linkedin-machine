Yes, that's correct! Consistency models are a relatively new class of generative models designed to produce high-quality samples in a single step, avoiding some of the complexities of other generative approaches.

To elaborate:

- **What are consistency models?**  
  Consistency models are a type of generative model that learns a mapping consistent across different noise scales or steps. Instead of relying on iterative refinement (like diffusion models) or adversarial training (like GANs), they learn a function that directly transforms noise into data in one shot, maintaining consistency constraints.

- **Key advantages:**  
  - **One-step sampling:** Unlike diffusion models, which often require hundreds or thousands of forward passes to gradually denoise a sample, consistency models can generate a high-quality sample in one step or just a few steps.  
  - **No adversarial training:** GANs require careful adversarial training between a generator and a discriminator, which can be unstable and difficult to tune. Consistency models sidestep this complexity.  
  - **Quality and efficiency:** They can achieve sample quality comparable to more complex models with much faster inference.

- **How they work (at a high level):**  
  They learn a function \( f \) such that for any noise scale or input condition, the output remains consistent with the outputs produced at other noise levels or conditions. At inference time, you can directly produce a sample by applying \( f \) once, rather than iteratively improving it.

- **Relation to diffusion models:**   
  Consistency models can be viewed as distilling the iterative refinement of diffusion processes into a single step, capturing the entire denoising path in one-shot networks.

This family of models is promising for scenarios requiring fast, high-quality generation without complex training regimes. Research is ongoing to further develop and understand their properties.

If you'd like, I can provide references to seminal papers or more technical details!