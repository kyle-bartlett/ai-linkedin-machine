To fine-tune a model with a single command, you typically use the command-line tools provided by the machine learning framework or platform you're using. Here's an example based on **OpenAI's fine-tuning CLI** and **Hugging Face's Transformers CLI**.

---

### 1. OpenAI CLI (for GPT models)

If you have your training data prepared as a JSONL file (`training_data.jsonl`), you can fine-tune GPT-3 models using a straightforward one-liner:

```bash
openai api fine_tunes.create -t "training_data.jsonl" -m "davinci"
```

This command:
- `-t` specifies the training file
- `-m` specifies the base model to fine-tune (e.g., `davinci`, `curie`)

---

### 2. Hugging Face Transformers CLI

For Hugging Face transformers, you usually write a script to fine-tune, but with **`transformers-cli`**, you can launch training from command line using the `run_clm.py` (causal language modeling) or other example scripts.

Example for fine-tuning GPT-2 on a single text file using `run_clm.py`:

```bash
python run_clm.py \
  --model_name_or_path gpt2 \
  --train_file path/to/train.txt \
  --validation_file path/to/valid.txt \
  --do_train \
  --do_eval \
  --output_dir ./fine_tuned_gpt2 \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \
  --num_train_epochs 3 \
  --save_steps 500
```

---

### Summary

- **OpenAI CLI:** One command with `openai api fine_tunes.create`
- **Hugging Face:** One command running a training script with required args

If you want me to generate a single command specific to your framework or dataset, please provide more details!