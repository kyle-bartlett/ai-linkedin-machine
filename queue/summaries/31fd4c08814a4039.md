The paper with arXiv ID 2511.16587v1 establishes new theoretical results on the long-run behavior of differentially private stochastic gradient descent (DP-SGD). The key contributions can be summarized as follows:

- **Almost Sure Convergence**: The authors prove that DP-SGD converges almost surely (i.e., with probability 1 along single trajectories), under standard smoothness assumptions on the objective function. This result applies both to nonconvex and strongly convex settings.

- **Step Size Conditions**: The convergence guarantees hold when the step sizes follow usual decaying rules commonly used in stochastic optimization.

- **Extension to Momentum-Based Methods**: The analysis is extended to privacy-preserving variants of momentum methods, including the stochastic heavy ball method (DP-SHB) and Nesterov's accelerated gradient (DP-NAG). Using energy-based analysis techniques, they show that these also enjoy almost sure convergence.

- **Implications**: These findings strengthen the theoretical foundation for privacy-preserving optimization by demonstrating that DP algorithms maintain pathwise stability despite the noise injected for privacy. This provides reassurance that differential privacy noise does not prevent the algorithm from converging reliably on individual runs, not just in expectation or probability.

In essence, this work closes a gap in the understanding of DP-SGD by moving beyond expected or high-probability convergence to a stronger notion of almost sure convergence of training trajectories, in both convex and nonconvex regimes, and including momentum variants.