Yes, that's correct. Diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs) and their variants, have achieved impressive results in generating high-quality and diverse images, audio, and videos by modeling the gradual denoising of data. However, the main limitation of these models is their inherently iterative sampling process, which involves starting from pure noise and repeatedly refining the output through many steps (sometimes hundreds or even thousands).

This iterative nature leads to slow generation times compared to other generative models like GANs (Generative Adversarial Networks) or autoregressive models that can often generate samples in a single or a few passes. Various research efforts have been made to accelerate diffusion models’ sampling speed, such as:

- **Improved Samplers**: Techniques like DDIM (Denoising Diffusion Implicit Models) reduce the number of steps needed while maintaining sample quality.
- **Knowledge Distillation**: Creating smaller models that approximate the original model’s output in fewer steps.
- **Hybrid Models**: Combining diffusion steps with other faster generative techniques.
- **Latent Diffusion**: Operating in a compressed latent space rather than pixel or raw data space to reduce computational cost.

Despite these advances, the trade-off between sample quality and generation speed remains a key challenge in applying diffusion models broadly in real-time or interactive applications.