That sounds like a fascinating approach! Evolved Policy Gradients (EPG) evolving the loss function itself is a promising direction for metalearning and could lead to more adaptable and sample-efficient agents. By allowing the loss to evolve, the agent’s learning dynamics can be tailored to better generalize to unseen tasks or variations, such as navigating to objects placed in novel positions.

If you’re looking for suggestions or help, here are some ideas or questions you might consider:

1. **Benchmarking and Evaluation:**  
   How do agents trained with EPG compare quantitatively against other metalearning methods like MAML, RL^2, or standard policy gradients on several benchmarks? It would be great to see performance not only on training tasks but also on significantly out-of-distribution tasks.

2. **Loss Function Insights:**  
   Have you analyzed the evolved loss functions to understand what structures or features are being favored? Sometimes inspecting the evolved objectives can offer intuition on what kind of inductive biases the method favors.

3. **Parameter Evolution Details:**  
   How is the loss function encoded and evolved? Is it parameterized as a neural network, a symbolic expression, or something else? How do you maintain stability and avoid degenerate solutions?

4. **Sample Efficiency:**  
   Does evolving the loss add overhead during training, or is it more computationally efficient in the long run due to faster adaptation on new tasks?

5. **Generalization to More Complex Domains:**  
   Have you tried extending EPG to more complex tasks, such as 3D navigation, manipulation, or multi-agent settings? Demonstrating scalability could increase impact.

6. **Open-Sourcing and Community:**  
   Are you planning on releasing code or environments to facilitate broader adoption?

If you’d like, I can help draft a technical summary or presentation, or provide feedback on any specific aspect. Just let me know!