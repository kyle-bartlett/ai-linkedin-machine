{
  "source": "HuggingFace Blog",
  "title": "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages",
  "url": "https://huggingface.co/blog/falcon2-11b",
  "published": "Fri, 24 May 2024 00:00:00 GMT",
  "summary_raw": "",
  "ingested_at": "2025-11-21 05:43:50.100509"
}