{
  "source": "OpenAI Blog",
  "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
  "url": "https://openai.com/index/the-instruction-hierarchy",
  "published": "Fri, 19 Apr 2024 19:00:00 GMT",
  "summary_raw": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts.",
  "ingested_at": "2025-11-21 05:43:48.548946"
}